{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling the sequential FPL team selection process as a Belief-State Markov Decision Process\n",
    "- For the $i$-th gameweek, we define the following terms:\n",
    "    - $M_i$ is set of matches in gameweek $i$.\n",
    "    - $P_i$ is the set of players available for selection in gameweek $i$.\n",
    "    - $A_i$ is the set of actions available in gameweek $i$, where $a \\in A_i$ is a subset of $P_i$ and observes all team selection constraints.\n",
    "    - $p_i \\in P_i$ is associated with its FPL-designated position $pos(p_i)$ and price $pr(p_i)$.\n",
    "    - $\\tau_p \\in \\tau$ is a system of distributions representing player's performance/influence on the matchplay.\n",
    "    - $O_i$ is the set of match observations in gameweek $i$\n",
    "    - $o \\in O_i$ includes both the result of the matches and the performance of the players in the selected team e.g. goals, assists, clean sheets, yellow cards, red cards, bonus points. The probability of each $o \\in O_i$ is somehow dependent on the players' characteristics ($\\tau$) i.e. a team with strong attackers is more likely to score goals, therefore, $P(o | \\tau)$ is dependent on $\\tau$.\n",
    "    - $R(o, a_{prev}, a_{curr})$ is the reward function, which returns the points scored by the selected team $a_{curr}$, given the match observations $o$. The previous team $a_{prev}$ is also provided to penalize the agent for any player poor player transfers or transfers beyond the allowed number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov's Decision Process (MDP) \n",
    "- A state $S_i$ would encapsulate\n",
    "    - $M_{i,..., 38}$ - set of upcoming fixtures for that gameweek\n",
    "    - $P_i$ - set of players available for selection\n",
    "    - $o \\in O_{i - 1}$ - the outcome of the previous gameweek\n",
    "    - $\\tau$ - the system of distributions representing players' abilities\n",
    "- An action $A_i$ is the set of teams selectable in gameweek $i$\n",
    "- $R$ is the corresponding reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Belief model ($\\tau$):\n",
    "- Represent uncertainty over players' abilities and generate samples $\\tau$ from the distribution $Pr(\\tau | b)$.\n",
    "- Three distributions are used to model the players' abilities:\n",
    "    - $\\rho_p$ - a three-state categorical distribution representing the player's probability of starting a match, being substituted, or not playing at all i.e. (start, sub, unused).\n",
    "    - $\\omega_p$ - a Bernoulli/Binomial distribution over a single trial, representing the probability of a player scoring a goal given he was playiong at the time\n",
    "    - $\\psi_p$ - a Bernoulli distribution representing the probability of a player providing an assist given he was playing at the time\n",
    "- Define prior distributions over the parameters of the above distributions and update them using the match observations $o$ to obtain new posterior distributions.\n",
    "- Use simple closed-form equations e.g. Beta and Dirichlet conjugate priors to update the priors.\n",
    "- Sample from these conjugate distributions to generate $\\tau_p$.\n",
    "- Define hyperparemeters uniformly across all players i.e. $$\\omega_p \\sim Beta(1, 1), \\psi_p \\sim Beta(1, 1),  \\rho_p \\sim Dirichlet(\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4})$$\n",
    "- Potential to use performance data from previous seasons to define priors\n",
    "- Define 4 global multinomial distributions $S_{pos}$ - one for each position - to describe the distribution of minutes players who play the same position $pos$ are likely to play in a match, given they start the match.\n",
    "- Player absence via injury/suspension or any other reson is modelled by setting the probability of starting and substituting to zero i.e. $Pr(\\rho_p = start) \\text{and} Pr(\\rho_p = sub) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formulating the belief-state MDP\n",
    "- The belief state at gameweek $i$, $b_i$, is an instantiation of our belief model, updated with the match observations $O_{i - 1}$.\n",
    "- We observe the posterior player characteristics by updating the belief state in response to an observation $o \\in O_i$ via the Bayes rule: $$Pr(\\tau | b_{i + 1}) \\propto Pr(o | \\tau)Pr(\\tau | b_i)$$\n",
    "- The agent can perform optimally by maximizing the value of the Bellman equation: $$V(b_i) = \\max_{a \\in A_i} Q(b_i, a)$$\n",
    "- The Q-function is defined as: $$Q(b_i, a) = \\int_{\\tau} Pr(\\tau | b_i)  \\int_{o \\in O_i} Pr(o | \\tau) \\left[r_i + \\gamma V(b_{i + 1}) \\right] \\text{dod}\\tau$$\n",
    "- Where:\n",
    "    - $\\gamma \\in [0, 1)$ is the discount factor for future rewards\n",
    "    - $r_i = R(o, a_{prev}, a)$ is the reward function\n",
    "    - $V(b_{i + 1})$ is the value of the next belief state\n",
    "- Solutions to the Bellman equation is intractable due to the size of the outcome space $|O_i|$, the size of the action space $|A_i|$, and the need to consider up to 38 gameweeks in order to calculate Q-values exactly.\n",
    "- We can work around this sampling from $O_i$ and simulating match outcomes to approximate the Q-function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling Outcomes\n",
    "- We describe a model for sampling outcomes for gameweek $i$ from $Pr(O_i | \\tau)$. This is then combined with the belief model described beforehand to obtain a joint distribution of player abilities and match outcomes, thus treating uncertainty in player abilities in a Bayesian manner (observations) $$Pr(O_i | \\tau)Pr(\\tau | b_i)$$\n",
    "- Sampling procedure for a single match that also extends to any other match in the gameweek (it also takes the perspective of the home team, which naturally extends to the away team as well):\n",
    "    - Define $P_H$ and $P_A$ as the set of players available the home and away teams respectively.\n",
    "    - Sample $\\tau_p$ for each player $p \\in P_H$  from the belief model $Pr(\\tau_p | b_i)$\n",
    "    - Randomly select eleven players from $P_H$ in proportion to their probability of starting the match i.e. $Pr(\\rho_p = start)$\n",
    "        - These players constitute the starting lineup $L_H$\n",
    "    - The minute each player leaves the pitch is sampled from the $S_{pos}$ distribution for the player's position\n",
    "    - Each player in $P_H$ and not in $L_H$ is assigned to the set of substitutes $U_H$\n",
    "        - At the start of each minute of the match, we check if any player in $L_H$ is scheduled to be substituted\n",
    "        - If so, we randomly select a player from $U_H$ to replace the outgoing player in proportion to the probability of the player being substituted i.e. $Pr(\\rho_p = sub)$\n",
    "        - The replacement is added to $L_H$ (removed from $U_H$). We further assume that the player being substituted is not substituted again in the same match.\n",
    "        - If a goal is scored according to the underlying team-based model, then it is allocated to player $p$ with probability $Pr(\\omega_p = 1)$ while an assist is allocated to player $p$ with probability $Pr(\\psi_p = 1)$.\n",
    "    - These point estimates may then be used in combination with the MDP reward function $R$ to approximate the immediate reward from performing any action, as well as to guide the exploration of high quality regions of the action space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesianFPLAgent import BayesianQLearningAgent\n",
    "from fpl_env import FPLEnv\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from constants import DATA_FOLDER\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytensor\n",
    "pytensor.config.cxx = \"/usr/bin/clang++\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env:FPLEnv, num_episodes: int) -> None:\n",
    "    \"\"\" Use the env provided to train the Bayesian Q-Learning agent on 2022-23 data\"\"\"\n",
    "\n",
    "    print(f\"Training agent for {num_episodes} episodes ...\")\n",
    "    start_time = datetime.now()\n",
    "    fantasy_agent = BayesianQLearningAgent()\n",
    "    episode_rewards = fantasy_agent.train(env, num_episodes=50)\n",
    "    print(f\"Training completed in {datetime.now() - start_time}\")\n",
    "\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env:FPLEnv, agent: BayesianQLearningAgent):\n",
    "    \"\"\"\n",
    "    Evaluate the trained agent against 2023-24 data\n",
    "    \"\"\"\n",
    "    print(\"Evaluating agent performance...\")\n",
    "    total_points, decisions = agent.evaluate(env)\n",
    "    \n",
    "    print(f\"Evaluation complete - Total points: {total_points}\")\n",
    "    \n",
    "    return total_points, decisions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(episode_rewards):\n",
    "    \"\"\"\n",
    "    Plot the learning curve of the agent\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward (Points)')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('learning_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Also plot cumulative rewards\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(np.cumsum(episode_rewards))\n",
    "    plt.title('Cumulative Reward')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Points')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('cumulative_reward.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player abilities matched players: 660\n",
      "Training agent for 50 episodes ...\n",
      "len: 777; unique: 777\n",
      "len: 777; unique: 777\n",
      "len: 777; unique: 777\n",
      "len: 777; unique: 777\n",
      "len: 777; unique: 777\n",
      "len: 777; unique: 777\n",
      "Actual team points: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [assist_beta, score_beta, start_sub_unused_dirichlet_dist]\n",
      "Sampling: [minutes_played, minutes_probs]\n",
      "Sampling: [assist_beta, score_beta, start_sub_unused_dirichlet_dist]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated fixture: Brighton(2) vs Man Utd(2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [minutes_played, minutes_probs]\n",
      "Sampling: [assist_beta, score_beta, start_sub_unused_dirichlet_dist]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated fixture: Crystal Palace(1) vs West Ham(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [minutes_played, minutes_probs]\n",
      "Sampling: [assist_beta, score_beta, start_sub_unused_dirichlet_dist]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated fixture: Fulham(0) vs Leicester(8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [minutes_played, minutes_probs]\n",
      "Sampling: [assist_beta, score_beta, start_sub_unused_dirichlet_dist]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated fixture: Man City(3) vs Leeds(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [minutes_played, minutes_probs]\n",
      "Sampling: [assist_beta, score_beta, start_sub_unused_dirichlet_dist]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated fixture: Southampton(2) vs Nott'm Forest(2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [minutes_played, minutes_probs]\n",
      "Sampling: [assist_beta, score_beta, start_sub_unused_dirichlet_dist]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated fixture: Spurs(1) vs Everton(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [minutes_played, minutes_probs]\n",
      "Sampling: [assist_beta, score_beta, start_sub_unused_dirichlet_dist]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated fixture: Aston Villa(2) vs Arsenal(3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [minutes_played, minutes_probs]\n",
      "Sampling: [assist_beta, score_beta, start_sub_unused_dirichlet_dist]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated fixture: Bournemouth(1) vs Newcastle(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [minutes_played, minutes_probs]\n",
      "Sampling: [assist_beta, score_beta, start_sub_unused_dirichlet_dist]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated fixture: Wolves(0) vs Chelsea(3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [minutes_played, minutes_probs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated fixture: Liverpool(1) vs Brentford(0)\n",
      "len: 777; unique: 777\n",
      "len: 777; unique: 777\n",
      "len: 777; unique: 777\n",
      "Actual team points: 16\n",
      "Fitting model with 23 teams:\n",
      "Arsenal, Aston Villa, Bournemouth, Brentford, Brighton, Burnley, Chelsea, Crystal Palace, Everton, Fulham, Leeds, Leicester, Liverpool, Man City, Man Utd, Newcastle, Norwich, Nott'm Forest, Southampton, Spurs, Watford, West Ham, Wolves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/munene/Desktop/YALE/Spring2025/CSEC491/FPL_AI/model/dixon_coles.py:82: RuntimeWarning: invalid value encountered in log\n",
      "  np.log(self._rho_correction(x, y, lambda_x, mu_y, rho) + epsilon)\n",
      "Sampling: [assist_beta, score_beta, start_sub_unused_dirichlet_dist]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 814.1952976233556\n",
      "            Iterations: 50\n",
      "            Function evaluations: 2523\n",
      "            Gradient evaluations: 50\n",
      "Imputed parameters for Luton using median values\n",
      "Imputed parameters for Sheffield Utd using median values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [minutes_played, minutes_probs]\n",
      "Sampling: [assist_beta, score_beta, start_sub_unused_dirichlet_dist]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated fixture: Arsenal(3) vs Brighton(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [minutes_played, minutes_probs]\n",
      "Sampling: [assist_beta, score_beta, start_sub_unused_dirichlet_dist]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated fixture: Brentford(2) vs Southampton(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [minutes_played, minutes_probs]\n",
      "Sampling: [assist_beta, score_beta, start_sub_unused_dirichlet_dist]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated fixture: Everton(0) vs Bournemouth(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [minutes_played, minutes_probs]\n",
      "Sampling: [assist_beta, score_beta, start_sub_unused_dirichlet_dist]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated fixture: Leeds(0) vs Fulham(4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [minutes_played, minutes_probs]\n",
      "Sampling: [assist_beta, score_beta, start_sub_unused_dirichlet_dist]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated fixture: Leicester(1) vs Aston Villa(6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [minutes_played, minutes_probs]\n",
      "Sampling: [assist_beta, score_beta, start_sub_unused_dirichlet_dist]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated fixture: Nott'm Forest(2) vs Wolves(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [minutes_played, minutes_probs]\n",
      "Sampling: [assist_beta, score_beta, start_sub_unused_dirichlet_dist]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated fixture: West Ham(0) vs Man City(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [minutes_played, minutes_probs]\n",
      "Sampling: [assist_beta, score_beta, start_sub_unused_dirichlet_dist]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated fixture: Chelsea(2) vs Crystal Palace(2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [minutes_played, minutes_probs]\n",
      "Sampling: [assist_beta, score_beta, start_sub_unused_dirichlet_dist]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated fixture: Newcastle(1) vs Spurs(2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [minutes_played, minutes_probs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated fixture: Man Utd(0) vs Liverpool(4)\n",
      "len: 777; unique: 777\n",
      "len: 777; unique: 777\n",
      "len: 777; unique: 777\n",
      "Actual team points: 24\n",
      "Fitting model with 23 teams:\n",
      "Arsenal, Aston Villa, Bournemouth, Brentford, Brighton, Burnley, Chelsea, Crystal Palace, Everton, Fulham, Leeds, Leicester, Liverpool, Man City, Man Utd, Newcastle, Norwich, Nott'm Forest, Southampton, Spurs, Watford, West Ham, Wolves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/munene/Desktop/YALE/Spring2025/CSEC491/FPL_AI/model/dixon_coles.py:82: RuntimeWarning: invalid value encountered in log\n",
      "  np.log(self._rho_correction(x, y, lambda_x, mu_y, rho) + epsilon)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     19\u001b[39m agent = BayesianQLearningAgent(\n\u001b[32m     20\u001b[39m     discount_factor=discount_factor,\n\u001b[32m     21\u001b[39m     search_depth=search_depth,\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     num_actions=\u001b[32m3\u001b[39m\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m  \u001b[38;5;66;03m# Train agent\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m episode_rewards = \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Plot learning curve\u001b[39;00m\n\u001b[32m     31\u001b[39m plot_learning_curve(episode_rewards)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mtrain_agent\u001b[39m\u001b[34m(env, num_episodes)\u001b[39m\n\u001b[32m      5\u001b[39m start_time = datetime.now()\n\u001b[32m      6\u001b[39m fantasy_agent = BayesianQLearningAgent()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m episode_rewards = \u001b[43mfantasy_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime.now()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m episode_rewards\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/YALE/Spring2025/CSEC491/FPL_AI/model/bayesianFPLAgent.py:252\u001b[39m, in \u001b[36mBayesianQLearningAgent.train\u001b[39m\u001b[34m(self, env, num_episodes)\u001b[39m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# Generate next candidate actions (if not done)\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_generate_candidate_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m     next_candidate_actions = env.candidate_actions\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/YALE/Spring2025/CSEC491/FPL_AI/model/fpl_env.py:349\u001b[39m, in \u001b[36mFPLEnv._generate_candidate_actions\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    346\u001b[39m fixtures_gw = \u001b[38;5;28mself\u001b[39m.fixtures_data[\u001b[38;5;28mself\u001b[39m.fixtures_data[\u001b[33m\"\u001b[39m\u001b[33mGW\u001b[39m\u001b[33m\"\u001b[39m] == next_gameweek]\n\u001b[32m    348\u001b[39m \u001b[38;5;66;03m# Simulate player points for the next gameweek\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m player_points_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_simulate_gameweek_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfixtures_gw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# Generate multiple teams using MKP\u001b[39;00m\n\u001b[32m    352\u001b[39m teams = generate_multiple_teams(\n\u001b[32m    353\u001b[39m     player_df=player_points_df,\n\u001b[32m    354\u001b[39m     num_teams=\u001b[32m3\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    358\u001b[39m     free_transfers=\u001b[38;5;28mself\u001b[39m.available_transfers\n\u001b[32m    359\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/YALE/Spring2025/CSEC491/FPL_AI/model/fpl_env.py:308\u001b[39m, in \u001b[36mFPLEnv._simulate_gameweek_points\u001b[39m\u001b[34m(self, fixtures_gw)\u001b[39m\n\u001b[32m    298\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    299\u001b[39m \u001b[33;03mSimulate player points for a gameweek\u001b[39;00m\n\u001b[32m    300\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    305\u001b[39m \u001b[33;03m    player_points: DataFrame with simulated points for each player\u001b[39;00m\n\u001b[32m    306\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    307\u001b[39m \u001b[38;5;66;03m# Use GameweekSimulator to get points\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m player_points = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgameweek_simulator\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimulate_gameweek\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgameweek\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_gameweek\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfixtures_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfixtures_gw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_minutes_df\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mposition_minutes_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplayers_ability_df\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mplayers_ability_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplayer_ability\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m player_points\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/YALE/Spring2025/CSEC491/FPL_AI/model/gameweek_simulator.py:71\u001b[39m, in \u001b[36mGameweekSimulator.simulate_gameweek\u001b[39m\u001b[34m(self, gameweek, fixtures_df, position_minutes_df, players_ability_df)\u001b[39m\n\u001b[32m     69\u001b[39m         dixon_coles_team_parameters = pickle.load(f)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     dixon_coles_team_parameters = \u001b[43mdixon_coles_prediction_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43msolve_parameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_season_start_year\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgameweek\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file=gameweek_parameters_file, mode=\u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     75\u001b[39m         pickle.dump(obj=dixon_coles_team_parameters, file=f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/YALE/Spring2025/CSEC491/FPL_AI/model/dixon_coles.py:220\u001b[39m, in \u001b[36mDixonColesModel.solve_parameters\u001b[39m\u001b[34m(self, season_start_year, gameweek)\u001b[39m\n\u001b[32m    217\u001b[39m constraints = [{\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33meq\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfun\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28msum\u001b[39m(x[:n_teams]) - n_teams}]\n\u001b[32m    219\u001b[39m \u001b[38;5;66;03m# Optimize\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m opt_output = \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimate_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_vals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxiter\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[38;5;66;03m# Create parameter dictionary\u001b[39;00m\n\u001b[32m    228\u001b[39m params = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mzip\u001b[39m(\n\u001b[32m    230\u001b[39m         [\u001b[33m\"\u001b[39m\u001b[33mattack_\u001b[39m\u001b[33m\"\u001b[39m + team \u001b[38;5;28;01mfor\u001b[39;00m team \u001b[38;5;129;01min\u001b[39;00m teams]\n\u001b[32m   (...)\u001b[39m\u001b[32m    234\u001b[39m     )\n\u001b[32m    235\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fpl_env/lib/python3.13/site-packages/scipy/optimize/_minimize.py:750\u001b[39m, in \u001b[36mminimize\u001b[39m\u001b[34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[39m\n\u001b[32m    747\u001b[39m     res = _minimize_cobyqa(fun, x0, args, bounds, constraints, callback,\n\u001b[32m    748\u001b[39m                            **options)\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33mslsqp\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m     res = \u001b[43m_minimize_slsqp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33mtrust-constr\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    753\u001b[39m     res = _minimize_trustregion_constr(fun, x0, args, jac, hess, hessp,\n\u001b[32m    754\u001b[39m                                        bounds, constraints,\n\u001b[32m    755\u001b[39m                                        callback=callback, **options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fpl_env/lib/python3.13/site-packages/scipy/optimize/_slsqp_py.py:435\u001b[39m, in \u001b[36m_minimize_slsqp\u001b[39m\u001b[34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, finite_diff_rel_step, **unknown_options)\u001b[39m\n\u001b[32m    429\u001b[39m slsqp(m, meq, x, xl, xu, fx, c, g, a, acc, majiter, mode, w, jw,\n\u001b[32m    430\u001b[39m       alpha, f0, gs, h1, h2, h3, h4, t, t0, tol,\n\u001b[32m    431\u001b[39m       iexact, incons, ireset, itermx, line,\n\u001b[32m    432\u001b[39m       n1, n2, n3)\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[32m1\u001b[39m:  \u001b[38;5;66;03m# objective and constraint evaluation required\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m     fx = \u001b[43mwrapped_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    436\u001b[39m     c = _eval_constraint(x, cons)\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == -\u001b[32m1\u001b[39m:  \u001b[38;5;66;03m# gradient evaluation required\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fpl_env/lib/python3.13/site-packages/scipy/optimize/_optimize.py:305\u001b[39m, in \u001b[36m_clip_x_for_func.<locals>.eval\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34meval\u001b[39m(x):\n\u001b[32m    304\u001b[39m     x = _check_clip_x(x, bounds)\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fpl_env/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:326\u001b[39m, in \u001b[36mScalarFunction.fun\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.array_equal(x, \u001b[38;5;28mself\u001b[39m.x):\n\u001b[32m    325\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_x(x)\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fpl_env/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:295\u001b[39m, in \u001b[36mScalarFunction._update_fun\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    294\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f_updated:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m         fx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wrapped_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m fx < \u001b[38;5;28mself\u001b[39m._lowest_f:\n\u001b[32m    297\u001b[39m             \u001b[38;5;28mself\u001b[39m._lowest_x = \u001b[38;5;28mself\u001b[39m.x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fpl_env/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:21\u001b[39m, in \u001b[36m_wrapper_fun.<locals>.wrapped\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     17\u001b[39m ncalls[\u001b[32m0\u001b[39m] += \u001b[32m1\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m fx = \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isscalar(fx):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/YALE/Spring2025/CSEC491/FPL_AI/model/dixon_coles.py:202\u001b[39m, in \u001b[36mDixonColesModel.solve_parameters.<locals>.estimate_parameters\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m    199\u001b[39m defend_coefs = \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(teams, params[n_teams : (\u001b[32m2\u001b[39m * n_teams)]))\n\u001b[32m    200\u001b[39m rho, gamma = params[-\u001b[32m2\u001b[39m:]\n\u001b[32m    201\u001b[39m log_like = [\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dc_log_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHomeGoals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAwayGoals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscore_coefs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHomeTeam\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdefend_coefs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHomeTeam\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscore_coefs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAwayTeam\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdefend_coefs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAwayTeam\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m fixture_results_df.itertuples()\n\u001b[32m    213\u001b[39m ]\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[38;5;28msum\u001b[39m(log_like)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/YALE/Spring2025/CSEC491/FPL_AI/model/dixon_coles.py:84\u001b[39m, in \u001b[36mDixonColesModel._dc_log_like\u001b[39m\u001b[34m(self, x, y, alpha_x, beta_x, alpha_y, beta_y, rho, gamma)\u001b[39m\n\u001b[32m     79\u001b[39m lambda_x, mu_y = np.exp(alpha_x + beta_y + gamma), np.exp(alpha_y + beta_x)\n\u001b[32m     80\u001b[39m epsilon = \u001b[32m1e-10\u001b[39m  \u001b[38;5;66;03m# Small value to prevent log(0)\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m     82\u001b[39m     np.log(\u001b[38;5;28mself\u001b[39m._rho_correction(x, y, lambda_x, mu_y, rho) + epsilon)\n\u001b[32m     83\u001b[39m     + np.log(poisson.pmf(x, lambda_x) + epsilon)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     + \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoisson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpmf\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "season_start_year = \"2022\"\n",
    "num_episodes = 50\n",
    "search_depth = 3  \n",
    "discount_factor = 0.5\n",
    "\n",
    "# Load 2022-23 season fixtures \n",
    "fixtures_2022_23 = pd.read_csv(filepath_or_buffer=f\"{DATA_FOLDER}/2023-24/fixtures.csv\")\n",
    "# Create environment\n",
    "env = FPLEnv(\n",
    "    season_start_year=season_start_year,\n",
    "    fixtures_data=fixtures_2022_23,\n",
    "    total_gameweeks=38,\n",
    "    current_gameweek=1,\n",
    "    budget=100.0,\n",
    "    render_mode=\"human\"\n",
    ")\n",
    "# Create agent\n",
    "agent = BayesianQLearningAgent(\n",
    "    discount_factor=discount_factor,\n",
    "    search_depth=search_depth,\n",
    "    init_variance_ratio=0.1,\n",
    "    episode_limit=num_episodes,\n",
    "    num_actions=3\n",
    ")\n",
    "\n",
    " # Train agent\n",
    "episode_rewards = train_agent(env, num_episodes)\n",
    "\n",
    "# Plot learning curve\n",
    "plot_learning_curve(episode_rewards)\n",
    "\n",
    "# Evaluate agent\n",
    "total_points, decisions = evaluate_agent(env, agent)\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'total_points': total_points,\n",
    "    'episode_rewards': episode_rewards,\n",
    "    'decisions': decisions\n",
    "}\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n=== Training Summary ===\")\n",
    "print(f\"Number of episodes: {num_episodes}\")\n",
    "print(f\"Discount factor: {discount_factor}\")\n",
    "print(f\"Search depth: {search_depth}\")\n",
    "print(f\"Average points per episode: {np.mean(episode_rewards):.2f}\")\n",
    "print(f\"Maximum points in an episode: {np.max(episode_rewards):.2f}\")\n",
    "\n",
    "print(\"\\n=== Evaluation ===\")\n",
    "print(f\"Total FPL points: {total_points}\")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fpl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
