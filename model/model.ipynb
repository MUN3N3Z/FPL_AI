{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling the sequential FPL team selection process as a Belief-State Markov Decision Process\n",
    "- For the $i$-th gameweek, we define the following terms:\n",
    "    - $M_i$ is set of matches in gameweek $i$.\n",
    "    - $P_i$ is the set of players available for selection in gameweek $i$.\n",
    "    - $A_i$ is the set of actions available in gameweek $i$, where $a \\in A_i$ is a subset of $P_i$ and observes all team selection constraints.\n",
    "    - $p_i \\in P_i$ is associated with its FPL-designated position $pos(p_i)$ and price $pr(p_i)$.\n",
    "    - $\\tau_p \\in \\tau$ is a system of distributions representing player's performance/influence on the matchplay.\n",
    "    - $O_i$ is the set of match observations in gameweek $i$\n",
    "    - $o \\in O_i$ includes both the result of the matches and the performance of the players in the selected team e.g. goals, assists, clean sheets, yellow cards, red cards, bonus points. The probability of each $o \\in O_i$ is somehow dependent on the players' characteristics ($\\tau$) i.e. a team with strong attackers is more likely to score goals, therefore, $P(o | \\tau)$ is dependent on $\\tau$.\n",
    "    - $R(o, a_{prev}, a_{curr})$ is the reward function, which returns the points scored by the selected team $a_{curr}$, given the match observations $o$. The previous team $a_{prev}$ is also provided to penalize the agent for any player poor player transfers or transfers beyond the allowed number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov's Decision Process (MDP) \n",
    "- A state $S_i$ would encapsulate\n",
    "    - $M_{i,..., 38}$ - set of upcoming fixtures for that gameweek\n",
    "    - $P_i$ - set of players available for selection\n",
    "    - $o \\in O_{i - 1}$ - the outcome of the previous gameweek\n",
    "    - $\\tau$ - the system of distributions representing players' abilities\n",
    "- An action $A_i$ is the set of teams selectable in gameweek $i$\n",
    "- $R$ is the corresponding reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Belief model ($\\tau$):\n",
    "- Represent uncertainty over players' abilities and generate samples $\\tau$ from the distribution $Pr(\\tau | b)$.\n",
    "- Three distributions are used to model the players' abilities:\n",
    "    - $\\rho_p$ - a three-state categorical distribution representing the player's probability of starting a match, being substituted, or not playing at all i.e. (start, sub, unused).\n",
    "    - $\\omega_p$ - a Bernoulli/Binomial distribution over a single trial, representing the probability of a player scoring a goal given he was playiong at the time\n",
    "    - $\\psi_p$ - a Bernoulli distribution representing the probability of a player providing an assist given he was playing at the time\n",
    "- Define prior distributions over the parameters of the above distributions and update them using the match observations $o$ to obtain new posterior distributions.\n",
    "- Use simple closed-form equations e.g. Beta and Dirichlet conjugate priors to update the priors.\n",
    "- Sample from these conjugate distributions to generate $\\tau_p$.\n",
    "- Define hyperparemeters uniformly across all players i.e. $$\\omega_p \\sim Beta(1, 1), \\psi_p \\sim Beta(1, 1),  \\rho_p \\sim Dirichlet(\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4})$$\n",
    "- Potential to use performance data from previous seasons to define priors\n",
    "- Define 4 global multinomial distributions $S_{pos}$ - one for each position - to describe the distribution of minutes players who play the same position $pos$ are likely to play in a match, given they start the match.\n",
    "- Player absence via injury/suspension or any other reson is modelled by setting the probability of starting and substituting to zero i.e. $Pr(\\rho_p = start) \\text{and} Pr(\\rho_p = sub) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formulating the belief-state MDP\n",
    "- The belief state at gameweek $i$, $b_i$, is an instantiation of our belief model, updated with the match observations $O_{i - 1}$.\n",
    "- We observe the posterior player characteristics by updating the belief state in response to an observation $o \\in O_i$ via the Bayes rule: $$Pr(\\tau | b_{i + 1}) \\propto Pr(o | \\tau)Pr(\\tau | b_i)$$\n",
    "- The agent can perform optimally by maximizing the value of the Bellman equation: $$V(b_i) = \\max_{a \\in A_i} Q(b_i, a)$$\n",
    "- The Q-function is defined as: $$Q(b_i, a) = \\int_{\\tau} Pr(\\tau | b_i)  \\int_{o \\in O_i} Pr(o | \\tau) \\left[r_i + \\gamma V(b_{i + 1}) \\right] \\text{dod}\\tau$$\n",
    "- Where:\n",
    "    - $\\gamma \\in [0, 1)$ is the discount factor for future rewards\n",
    "    - $r_i = R(o, a_{prev}, a)$ is the reward function\n",
    "    - $V(b_{i + 1})$ is the value of the next belief state\n",
    "- Solutions to the Bellman equation is intractable due to the size of the outcome space $|O_i|$, the size of the action space $|A_i|$, and the need to consider up to 38 gameweeks in order to calculate Q-values exactly.\n",
    "- We can work around this sampling from $O_i$ and simulating match outcomes to approximate the Q-function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling Outcomes\n",
    "- We describe a model for sampling outcomes for gameweek $i$ from $Pr(O_i | \\tau)$. This is then combined with the belief model described beforehand to obtain a joint distribution of player abilities and match outcomes, thus treating uncertainty in player abilities in a Bayesian manner (observations) $$Pr(O_i | \\tau)Pr(\\tau | b_i)$$\n",
    "- Sampling procedure for a single match that also extends to any other match in the gameweek (it also takes the perspective of the home team, which naturally extends to the away team as well):\n",
    "    - Define $P_H$ and $P_A$ as the set of players available the home and away teams respectively.\n",
    "    - Sample $\\tau_p$ for each player $p \\in P_H$  from the belief model $Pr(\\tau_p | b_i)$\n",
    "    - Randomly select eleven players from $P_H$ in proportion to their probability of starting the match i.e. $Pr(\\rho_p = start)$\n",
    "        - These players constitute the starting lineup $L_H$\n",
    "    - The minute each player leaves the pitch is sampled from the $S_{pos}$ distribution for the player's position\n",
    "    - Each player in $P_H$ and not in $L_H$ is assigned to the set of substitutes $U_H$\n",
    "        - At the start of each minute of the match, we check if any player in $L_H$ is scheduled to be substituted\n",
    "        - If so, we randomly select a player from $U_H$ to replace the outgoing player in proportion to the probability of the player being substituted i.e. $Pr(\\rho_p = sub)$\n",
    "        - The replacement is added to $L_H$ (removed from $U_H$). We further assume that the player being substituted is not substituted again in the same match.\n",
    "        - If a goal is scored according to the underlying team-based model, then it is allocated to player $p$ with probability $Pr(\\omega_p = 1)$ while an assist is allocated to player $p$ with probability $Pr(\\psi_p = 1)$.\n",
    "    - These point estimates may then be used in combination with the MDP reward function $R$ to approximate the immediate reward from performing any action, as well as to guide the exploration of high quality regions of the action space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from constants import GAMEWEEK_COUNT, DATA_FOLDER\n",
    "from gameweek_simulator import GameweekSimulator\n",
    "from player_ability import PlayerAbility\n",
    "from position_minutes import PositionMinutesModel\n",
    "from fpl_env import FPLEnv\n",
    "from bayesianFPLAgent import BayesianFPLAgent\n",
    "from pprint import pprint\n",
    "from utils import update_gw_data\n",
    "from data_registry import DataRegistry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player abilities matched players: 585\n",
      "                        name                     name.1  team position  \\\n",
      "0      Rasmus HÃÂÃÂ¸jlund      Rasmus HÃÂÃÂ¸jlund    14      FWD   \n",
      "1  Arnaut Danjuma Groeneveld  Arnaut Danjuma Groeneveld     9      MID   \n",
      "2      Dominic Calvert-Lewin      Dominic Calvert-Lewin     9      FWD   \n",
      "3                Neal Maupay                Neal Maupay     9      FWD   \n",
      "4                  Luke Shaw                  Luke Shaw    14      DEF   \n",
      "\n",
      "                      ρ_β   ω_α    ω_β   ψ_α    ψ_β  start_prob  sub_prob  \\\n",
      "0        [0.25 0.25 0.5 ]   0.0    5.0   0.0    5.0    0.247581  0.252032   \n",
      "1     [ 5.25  9.25 24.5 ]   0.0   19.0   1.0   18.0    0.134817  0.235944   \n",
      "2  [128.25  53.25  76.5 ]  47.0  139.0  23.0  163.0    0.497382  0.206326   \n",
      "3     [88.25 41.25 23.5 ]  27.0  107.0   8.0  126.0    0.576885  0.269797   \n",
      "4  [132.25  26.25 108.5 ]   3.0  160.0  20.0  143.0    0.495006  0.098539   \n",
      "\n",
      "   unused_prob  score_prob  assist_prob  sampled_points  minutes_played  \n",
      "0     0.500387    0.000107     0.000237               1               9  \n",
      "1     0.629239    0.000040     0.052262               1               9  \n",
      "2     0.296292    0.252387     0.123965              15               8  \n",
      "3     0.153318    0.201463     0.059667               1               8  \n",
      "4     0.406455    0.018506     0.123151               1               8  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sampled_points'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/var/folders/c1/fv0lp6t93sb5n0g1zjq_6_y40000gn/T/ipykernel_6094/1678941030.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     41\u001b[39m         cumulative_real_player_points=cumulative_real_player_points,\n\u001b[32m     42\u001b[39m         gw=gw_count,\n\u001b[32m     43\u001b[39m     )\n\u001b[32m     44\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     reinforcement_learning_env = FPLEnv(\n\u001b[32m     46\u001b[39m         player_performance_samples=player_points_df,\n\u001b[32m     47\u001b[39m         initial_budget=\u001b[32m100.0\u001b[39m,\n\u001b[32m     48\u001b[39m         max_transfers_per_gw=\u001b[32m1\u001b[39m,\n",
      "\u001b[32m~/Desktop/YALE/Spring2025/CSEC491/FPL_AI/model/fpl_env.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, player_performance_samples, initial_budget, max_transfers_per_gw, transfer_penalty)\u001b[39m\n\u001b[32m     67\u001b[39m                 ),\n\u001b[32m     68\u001b[39m             }\n\u001b[32m     69\u001b[39m         )\n\u001b[32m     70\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m         self.reset(seed=RANDOM_SEED)\n",
      "\u001b[32m~/Desktop/YALE/Spring2025/CSEC491/FPL_AI/model/fpl_env.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m     78\u001b[39m         self._total_points = \u001b[32m0\u001b[39m\n\u001b[32m     79\u001b[39m         self._free_transfers = \u001b[32m1\u001b[39m\n\u001b[32m     80\u001b[39m \n\u001b[32m     81\u001b[39m         self._team = self._initialize_team()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m         self._bench = self._select_bench()\n\u001b[32m     83\u001b[39m         self._playing = self._team[~self._team[\u001b[33m\"name\"\u001b[39m].isin(self._bench[\u001b[33m\"name\"\u001b[39m])]\n\u001b[32m     84\u001b[39m         self._captain = self._select_captain()\n\u001b[32m     85\u001b[39m         self._vice_captain = self._select_vice_captain()\n",
      "\u001b[32m~/Desktop/YALE/Spring2025/CSEC491/FPL_AI/model/fpl_env.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    203\u001b[39m             - At least \u001b[32m3\u001b[39m DEF\n\u001b[32m    204\u001b[39m             - At least \u001b[32m3\u001b[39m MID\n\u001b[32m    205\u001b[39m             - At least \u001b[32m1\u001b[39m FWD\n\u001b[32m    206\u001b[39m         \"\"\"\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m         sorted_team_descending = self._team.sort_values(\n\u001b[32m    208\u001b[39m             by=\u001b[33m\"sampled_points\"\u001b[39m, ascending=\u001b[38;5;28;01mTrue\u001b[39;00m, inplace=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    209\u001b[39m         )\n\u001b[32m    210\u001b[39m         bench = []\n",
      "\u001b[32m~/miniconda3/envs/fpl_env/lib/python3.13/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[39m\n\u001b[32m   7185\u001b[39m             )\n\u001b[32m   7186\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m len(by):\n\u001b[32m   7187\u001b[39m             \u001b[38;5;66;03m# len(by) == 1\u001b[39;00m\n\u001b[32m   7188\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m7189\u001b[39m             k = self._get_label_or_level_values(by[\u001b[32m0\u001b[39m], axis=axis)\n\u001b[32m   7190\u001b[39m \n\u001b[32m   7191\u001b[39m             \u001b[38;5;66;03m# need to rewrap column in Series to apply key function\u001b[39;00m\n\u001b[32m   7192\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m~/miniconda3/envs/fpl_env/lib/python3.13/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'sampled_points'"
     ]
    }
   ],
   "source": [
    "SEASON_START_YEAR = \"2023\"\n",
    "\n",
    "fixtures_2023_24 = pd.read_csv(filepath_or_buffer=f\"{DATA_FOLDER}/2023-24/fixtures.csv\")\n",
    "gameweek_data = DataRegistry(gw_data_columns=[\"name\", \"total_points\", \"value\", \"GW\"]).gameweek_data[\"2023-24\"]\n",
    "\n",
    "\n",
    "# Bayesian model for players' ability to (start, get subbed, or not play), score, and assist\n",
    "player_ability_model = PlayerAbility(season_start_year=SEASON_START_YEAR, gameweek=\"0\")\n",
    "\n",
    "# Bayesian model for players' game minutes per position \n",
    "position_minutes_model = PositionMinutesModel()\n",
    "\n",
    "cumulative_real_player_points = {\n",
    "    player_name: 0\n",
    "    for player_name in player_ability_model.player_ability.name\n",
    "}\n",
    "\n",
    "for gw_count in range(1, GAMEWEEK_COUNT + 1, 1):\n",
    "    # Filter fixtures and players available for specified gameweek\n",
    "    fixtures_2023_24_gameweek = fixtures_2023_24[fixtures_2023_24[\"GW\"] == gw_count]\n",
    "    # Small caveat for GW since no previous gameweek to reference for prices only; not points (will be 0)\n",
    "    gameweek_data_current_gw = gameweek_data[gameweek_data[\"GW\"] == gw_count] if (\n",
    "        gw_count == 1) else gameweek_data[gameweek_data[\"GW\"] == (gw_count - 1)]\n",
    "\n",
    "    # player_points_df = GameweekSimulator().simulate_gameweek(\n",
    "    #     season_start_year=SEASON_START_YEAR, \n",
    "    #     gameweek=str(gw_count),\n",
    "    #     fixtures_df=fixtures_2023_24_gameweek,\n",
    "    #     players_ability_df=player_ability_model.player_ability[\n",
    "    #         player_ability_model.player_ability.name.isin(gameweek_data_current_gw.name)\n",
    "    #     ],\n",
    "    #     position_minutes_df=position_minutes_model.model_df\n",
    "    # )\n",
    "    # player_points_df.to_csv(\"../data/2023-24/model_data/player_points_df.csv\")\n",
    "    player_points_df = pd.read_csv(\"../data/2023-24/model_data/player_points_df.csv\")\n",
    "    print(player_points_df.head(5))\n",
    "    # Update players' cumulative real points\n",
    "    player_points_df, cumulative_real_player_points = update_gw_data(\n",
    "        gameweek_data=gameweek_data_current_gw,\n",
    "        player_points_df=player_points_df, \n",
    "        cumulative_real_player_points=cumulative_real_player_points,\n",
    "        gw=gw_count,\n",
    "    )\n",
    "\n",
    "    reinforcement_learning_env = FPLEnv(\n",
    "        player_performance_samples=player_points_df,\n",
    "        initial_budget=100.0,\n",
    "        max_transfers_per_gw=1,\n",
    "        transfer_penalty=4\n",
    "    )\n",
    "    agent = BayesianFPLAgent()\n",
    "    results, state_info = agent.train(env=reinforcement_learning_env, num_episodes=1000)\n",
    "    pprint(state_info)\n",
    "    print(f\"Gameweek: {gw_count}; Projected points: {results[-1]}\")\n",
    "    print(\"---------------------------------------------------------------------------------------\")\n",
    "    # Update player ability priors with completed gameweek's data\n",
    "    player_ability_model.update_model()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fpl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
