{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling the sequential FPL team selection process as a Belief-State Markov Decision Process\n",
    "- For the $i$-th gameweek, we define the following terms:\n",
    "    - $M_i$ is set of matches in gameweek $i$.\n",
    "    - $P_i$ is the set of players available for selection in gameweek $i$.\n",
    "    - $A_i$ is the set of actions available in gameweek $i$, where $a \\in A_i$ is a subset of $P_i$ and observes all team selection constraints.\n",
    "    - $p_i \\in P_i$ is associated with its FPL-designated position $pos(p_i)$ and price $pr(p_i)$.\n",
    "    - $\\tau_p \\in \\tau$ is a system of distributions representing player's performance/influence on the matchplay.\n",
    "    - $O_i$ is the set of match observations in gameweek $i$\n",
    "    - $o \\in O_i$ includes both the result of the matches and the performance of the players in the selected team e.g. goals, assists, clean sheets, yellow cards, red cards, bonus points. The probability of each $o \\in O_i$ is somehow dependent on the players' characteristics ($\\tau$) i.e. a team with strong attackers is more likely to score goals, therefore, $P(o | \\tau)$ is dependent on $\\tau$.\n",
    "    - $R(o, a_{prev}, a_{curr})$ is the reward function, which returns the points scored by the selected team $a_{curr}$, given the match observations $o$. The previous team $a_{prev}$ is also provided to penalize the agent for any player poor player transfers or transfers beyond the allowed number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov's Decision Process (MDP) \n",
    "- A state $S_i$ would encapsulate\n",
    "    - $M_{i,..., 38}$ - set of upcoming fixtures for that gameweek\n",
    "    - $P_i$ - set of players available for selection\n",
    "    - $o \\in O_{i - 1}$ - the outcome of the previous gameweek\n",
    "    - $\\tau$ - the system of distributions representing players' abilities\n",
    "- An action $A_i$ is the set of teams selectable in gameweek $i$\n",
    "- $R$ is the corresponding reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Belief model ($\\tau$):\n",
    "- Represent uncertainty over players' abilities and generate samples $\\tau$ from the distribution $Pr(\\tau | b)$.\n",
    "- Three distributions are used to model the players' abilities:\n",
    "    - $\\rho_p$ - a three-state categorical distribution representing the player's probability of starting a match, being substituted, or not playing at all i.e. (start, sub, unused).\n",
    "    - $\\omega_p$ - a Bernoulli/Binomial distribution over a single trial, representing the probability of a player scoring a goal given he was playiong at the time\n",
    "    - $\\psi_p$ - a Bernoulli distribution representing the probability of a player providing an assist given he was playing at the time\n",
    "- Define prior distributions over the parameters of the above distributions and update them using the match observations $o$ to obtain new posterior distributions.\n",
    "- Use simple closed-form equations e.g. Beta and Dirichlet conjugate priors to update the priors.\n",
    "- Sample from these conjugate distributions to generate $\\tau_p$.\n",
    "- Define hyperparemeters uniformly across all players i.e. $$\\omega_p \\sim Beta(1, 1), \\psi_p \\sim Beta(1, 1),  \\rho_p \\sim Dirichlet(\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4})$$\n",
    "- Potential to use performance data from previous seasons to define priors\n",
    "- Define 4 global multinomial distributions $S_{pos}$ - one for each position - to describe the distribution of minutes players who play the same position $pos$ are likely to play in a match, given they start the match.\n",
    "- Player absence via injury/suspension or any other reson is modelled by setting the probability of starting and substituting to zero i.e. $Pr(\\rho_p = start) \\text{and} Pr(\\rho_p = sub) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized gameweek data saved to csv!\n"
     ]
    }
   ],
   "source": [
    "from utils.data_registry import DataRegistry\n",
    "import pymc as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arviz as az\n",
    "\n",
    "# There are 38 gameweeks in a season\n",
    "GAMEWEEK_COUNT = 38\n",
    "\n",
    "seasonal_gameweek_player_data = DataRegistry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_player_ability_priors():\n",
    "    # Define player ability (only for players in the 2023/24 season) priors to reflect the occurrences of previous seasons i.e. 2016/17 - 2022/23\n",
    "    player_ability_df = pd.DataFrame({\n",
    "        \"name\": seasonal_gameweek_player_data.player_data[\"2023-24\"][\"first_name\"] + \" \" + seasonal_gameweek_player_data.player_data[\"2023-24\"][\"second_name\"],\n",
    "        \"ρ_β\": [np.array([1/4, 1/4, 1/4])] * len(seasonal_gameweek_player_data.player_data[\"2023-24\"]), # Dirichlet prior for ρ\n",
    "        (\"ω\", \"α\"): np.ones(len(seasonal_gameweek_player_data.player_data[\"2023-24\"])), # Beta prior for ω\n",
    "        (\"ω\", \"β\"): np.ones(len(seasonal_gameweek_player_data.player_data[\"2023-24\"])), # Beta prior for ω\n",
    "        (\"ψ\", \"α\"): np.ones(len(seasonal_gameweek_player_data.player_data[\"2023-24\"])), # Beta prior for ψ\n",
    "        (\"ψ\", \"β\"): np.ones(len(seasonal_gameweek_player_data.player_data[\"2023-24\"])) # Beta prior for ψ\n",
    "    })\n",
    "    player_priors = {\n",
    "        player_name: {\n",
    "            \"ρ_β\": np.array([1/4, 1/4, 1/4]),\n",
    "            \"ω_α\": 1.0,\n",
    "            \"ω_β\": 1.0,\n",
    "            \"ψ_α\": 1.0,\n",
    "            \"ψ_β\": 1.0\n",
    "        }\n",
    "        for player_name in player_ability_df[\"name\"]\n",
    "    }\n",
    "\n",
    "    for season, gameweek_data_df in seasonal_gameweek_player_data.gameweek_data.items():\n",
    "        for gameweek_count in range(1, GAMEWEEK_COUNT + 1):\n",
    "            # Filter out players not present in the 2023/24 season\n",
    "            gameweek_data_df = gameweek_data_df[gameweek_data_df[\"name\"].isin(player_ability_df[\"name\"])]\n",
    "            grouped_data = gameweek_data_df.groupby([\"name\", \"GW\"])\n",
    "            for (player_name, _), player_gameweek_data in grouped_data:\n",
    "                priors = player_priors[player_name]\n",
    "                with pm.Model() as sequential_player_ability_model:\n",
    "\n",
    "                    # Priors for the current iteration\n",
    "                    ρ = pm.Dirichlet('ρ', a=priors[\"ρ_β\"])\n",
    "                    ω = pm.Beta('ω', alpha=priors[\"ω_α\"], beta=priors[\"ω_β\"])\n",
    "                    ψ = pm.Beta('ψ', alpha=priors[\"ψ_α\"], beta=priors[\"ψ_β\"])\n",
    "\n",
    "                    # ρ Likelihood (multinomial) -> (played, subbed, not_used)\n",
    "                    ρ_observed = np.zeros(3)\n",
    "                    if player_gameweek_data[\"minutes\"].iloc[0] == 0:\n",
    "                        ρ_observed[2] = 1\n",
    "                    else:\n",
    "                        if player_gameweek_data[\"starts\"].iloc[0] == 1:\n",
    "                            ρ_observed[0] = 1\n",
    "                        else:\n",
    "                            ρ_observed[1] = 1\n",
    "                    ρ_likelihood = pm.Multinomial(name=\"ρ_likelihood\", n=np.sum(ρ_observed), p=ρ, observed=ρ_observed)\n",
    "                    # ω Likelihood (binomial)\n",
    "                    ω_observed = player_gameweek_data[\"goals_scored\"].sum()\n",
    "                    ω_likelihood = pm.Binomial(name=\"ω_observed\", n=ω_observed, p=ω, observed=ω_observed)\n",
    "                    # ψ Likelihood (binomial)\n",
    "                    ψ_observed = player_gameweek_data[\"assists\"].sum()\n",
    "                    ψ_likelihood = pm.Binomial(name=\"ψ_observed\", n=ψ_observed, p=ψ, observed=ψ_observed)\n",
    "\n",
    "                    # Sample the posterior\n",
    "                    trace = pm.sample(draws=1000, tune=500, chains=2)\n",
    "                \n",
    "                    # Update priors for the next iteration\n",
    "                    posterior_means = az.summary(trace, var_names=[\"ρ\", \"ω\", \"ψ\"])\n",
    "                    priors[\"ρ_β\"] = posterior_means.loc[\"ρ_β\", \"mean\"][:3]\n",
    "                    priors[\"ω_α\"] = posterior_means.loc[:, 'mean'][3]\n",
    "                    priors[\"ω_β\"] = 1 - priors[\"ω_α\"]\n",
    "                    priors[\"ψ_α\"] = posterior_means.loc[:, 'mean'][4]\n",
    "                    priors[\"ψ_β\"] = 1 - priors[\"ψ_α\"]\n",
    "                print(f\"{player_name}\")\n",
    "                # Save intermediate results every 5 gameweeks\n",
    "            if gameweek_count % 5 == 0:\n",
    "                player_ability_df.to_csv(path_or_buf=f\"./data/player_ability_results/player_ability_gw_{gameweek_count}.csv\", index=False)\n",
    "\n",
    "    # Update player_ability_df with the updated priors from player_priors\n",
    "    for player_name, priors in player_priors.items():\n",
    "        player_ability_df.loc[player_ability_df[\"name\"] == player_name, \"ρ_β\"] = [priors[\"ρ_β\"]]\n",
    "        player_ability_df.loc[player_ability_df[\"name\"] == player_name, (\"ω\", \"α\")] = priors[\"ω_α\"]\n",
    "        player_ability_df.loc[player_ability_df[\"name\"] == player_name, (\"ω\", \"β\")] = priors[\"ω_β\"]\n",
    "        player_ability_df.loc[player_ability_df[\"name\"] == player_name, (\"ψ\", \"α\")] = priors[\"ψ_α\"]\n",
    "        player_ability_df.loc[player_ability_df[\"name\"] == player_name, (\"ψ\", \"β\")] = priors[\"ψ_β\"]\n",
    "    # Save player_ability_df as a csv file\n",
    "    player_ability_df.to_csv(path_or_buf=\"./data/player_ability.csv\", index=False)\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_player_position_minutes_priors():\n",
    "    # Global multinomial distributions for players' minutes played at each position\n",
    "    player_positions = seasonal_gameweek_player_data.player_data[\"2023-24\"][\"element_type\"].unique()\n",
    "    player_position_minutes_df = pd.DataFrame({\n",
    "        \"position\": player_positions,\n",
    "        \"minutes\": [np.zeros(91) for _ in range(len(player_positions))] # Dirichlet prior for minutes at each position\n",
    "    })\n",
    "    seasons_with_player_position_data = {\"2020-21\", \"2021-22\", \"2022-23\"}\n",
    "    # Use a dictionary for faster indexing in the for loop\n",
    "    position_priors = {\n",
    "        position: np.ones(91)\n",
    "        for position in player_positions\n",
    "    }\n",
    "    for season, gameweek_data_df in seasonal_gameweek_player_data.gameweek_data.items():\n",
    "        if season in seasons_with_player_position_data:\n",
    "            group_data = gameweek_data_df.groupby([\"position\"])\n",
    "            for (position,), group_df in group_data:\n",
    "                # Aggregate observed minutes played for the position\n",
    "                observed_minutes = np.zeros(91)\n",
    "                for minutes_played in group_df[\"minutes\"]:\n",
    "                    if 0 <= minutes_played <= 90:\n",
    "                        observed_minutes[int(minutes_played)] += 1\n",
    "                with pm.Model() as player_position_minutes_played_model:\n",
    "                    # Dirichlet prior for minutes played at each position\n",
    "                    print(season)\n",
    "                    prior = pm.Dirichlet(position, a=position_priors[position], shape=91)\n",
    "                    likelihood = pm.Multinomial(\n",
    "                        name=f\"likelihood_{position}\", \n",
    "                        n=np.sum(observed_minutes),\n",
    "                        p=prior,\n",
    "                        observed=observed_minutes)\n",
    "                    # Sample the posterior\n",
    "                    trace = pm.sample(draws=1000, tune=500, chains=2)\n",
    "                    # Update priors for the next iteration\n",
    "                    posterior_means = az.summary(trace, var_names=[position]).loc[:, \"mean\"].values\n",
    "                    position_priors[position] = posterior_means\n",
    "    # Transfer priors to player_position_minutes_df\n",
    "    for position, priors in position_priors.items():\n",
    "        player_position_minutes_df.loc[player_position_minutes_df[\"position\"] == position, \"minutes\"] = [priors]\n",
    "    # Save player_position_minutes_df as a csv file\n",
    "    player_position_minutes_df.to_csv(path_or_buf=\"./data/player_position_minutes.csv\", index=False)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [DEF]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6f344379c3427aad4c53dbfbc344b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 2 chains for 1_000 tune and 620 draw iterations (2_000 + 1_240 draws total) took 74 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [FWD]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3708830aafd741589904c458c39241cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_player_position_minutes_priors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formulating the belief-state MDP\n",
    "- The belief state at gameweek $i$, $b_i$, is an instantiation of our belief model, updated with the match observations $O_{i - 1}$.\n",
    "- We observe the posterior player characteristics by updating the belief state in response to an observation $o \\in O_i$ via the Bayes rule: $$Pr(\\tau | b_{i + 1}) \\propto Pr(o | \\tau)Pr(\\tau | b_i)$$\n",
    "- The agent can perform optimally by maximizing the value of the Bellman equation: $$V(b_i) = \\max_{a \\in A_i} Q(b_i, a)$$\n",
    "- The Q-function is defined as: $$Q(b_i, a) = \\int_{\\tau} Pr(\\tau | b_i)  \\int_{o \\in O_i} Pr(o | \\tau) \\left[r_i + \\gamma V(b_{i + 1}) \\right] \\text{dod}\\tau$$\n",
    "- Where:\n",
    "    - $\\gamma \\in [0, 1)$ is the discount factor for future rewards\n",
    "    - $r_i = R(o, a_{prev}, a)$ is the reward function\n",
    "    - $V(b_{i + 1})$ is the value of the next belief state\n",
    "- Solutions to the Bellman equation is intractable due to the size of the outcome space $|O_i|$, the size of the action space $|A_i|$, and the need to consider up to 38 gameweeks in order to calculate Q-values exactly.\n",
    "- We can work around this sampling from $O_i$ and simulating match outcomes to approximate the Q-function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling Outcomes\n",
    "- We describe a model for sampling outcomes for gameweek $i$ from $Pr(O_i | \\tau)$. This is then combined with the belief model described beforehand to obtain a joint distribution of player abilities and match outcomes, thus treating uncertainty in player abilities in a Bayesian manner (observations) $$Pr(O_i | \\tau)Pr(\\tau | b_i)$$\n",
    "- Sampling procedure for a single match that also extends to any other match in the gameweek (it also takes the perspective of the home team, which naturally extends to the away team as well):\n",
    "    - Define $P_H$ and $P_A$ as the set of players available the home and away teams respectively.\n",
    "    - Sample $\\tau_p$ for each player $p \\in P_H  from the belief model $Pr(\\tau_p | b_i)$\n",
    "    - Randomly select eleven players from $P_H$ in proportion to their probability of starting the match i.e. $Pr(\\rho_p = start)$\n",
    "        - These players constitute the starting lineup $L_H$\n",
    "    - The minute each player leaves the pitch is sampled from the $S_{pos}$ distribution for the player's position\n",
    "    - Each player in $P_H$ and not in $L_H$ is assigned to the set of substitutes $U_H$\n",
    "        - At the start of each minute of the match, we check if any player in $L_H$ is scheduled to be substituted\n",
    "        - If so, we randomly select a player from $U_H$ to replace the outgoing player in proportion to the probability of the player being substituted i.e. $Pr(\\rho_p = sub)$\n",
    "        - The replacement is added to $L_H$ (removed from $U_H$). We further assume that the player being substituted is not substituted again in the same match.\n",
    "        - If a goal is scored according to the underlying team-based model, then it is allocated to player $p$ with probability $Pr(\\omega_p = 1)$ while an assist is allocated to player $p$ with probability $Pr(\\psi_p = 1)$.\n",
    "    - These point estimates may then be used in combination with the MDP reward function $R$ to approximate the immediate reward from performing any action, as well as to guide the exploration of high quality regions of the action space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
