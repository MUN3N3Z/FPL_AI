{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling the sequential FPL team selection process as a Belief-State Markov Decision Process\n",
    "- For the $i$-th gameweek, we define the following terms:\n",
    "    - $M_i$ is set of matches in gameweek $i$.\n",
    "    - $P_i$ is the set of players available for selection in gameweek $i$.\n",
    "    - $A_i$ is the set of actions available in gameweek $i$, where $a \\in A_i$ is a subset of $P_i$ and observes all team selection constraints.\n",
    "    - $p_i \\in P_i$ is associated with its FPL-designated position $pos(p_i)$ and price $pr(p_i)$.\n",
    "    - $\\tau_p \\in \\tau$ is a system of distributions representing player's performance/influence on the matchplay.\n",
    "    - $O_i$ is the set of match observations in gameweek $i$\n",
    "    - $o \\in O_i$ includes both the result of the matches and the performance of the players in the selected team e.g. goals, assists, clean sheets, yellow cards, red cards, bonus points. The probability of each $o \\in O_i$ is somehow dependent on the players' characteristics ($\\tau$) i.e. a team with strong attackers is more likely to score goals, therefore, $P(o | \\tau)$ is dependent on $\\tau$.\n",
    "    - $R(o, a_{prev}, a_{curr})$ is the reward function, which returns the points scored by the selected team $a_{curr}$, given the match observations $o$. The previous team $a_{prev}$ is also provided to penalize the agent for any player poor player transfers or transfers beyond the allowed number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov's Decision Process (MDP) \n",
    "- A state $S_i$ would encapsulate\n",
    "    - $M_{i,..., 38}$ - set of upcoming fixtures for that gameweek\n",
    "    - $P_i$ - set of players available for selection\n",
    "    - $o \\in O_{i - 1}$ - the outcome of the previous gameweek\n",
    "    - $\\tau$ - the system of distributions representing players' abilities\n",
    "- An action $A_i$ is the set of teams selectable in gameweek $i$\n",
    "- $R$ is the corresponding reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Belief model ($\\tau$):\n",
    "- Represent uncertainty over players' abilities and generate samples $\\tau$ from the distribution $Pr(\\tau | b)$.\n",
    "- Three distributions are used to model the players' abilities:\n",
    "    - $\\rho_p$ - a three-state categorical distribution representing the player's probability of starting a match, being substituted, or not playing at all i.e. (start, sub, unused).\n",
    "    - $\\omega_p$ - a Bernoulli/Binomial distribution over a single trial, representing the probability of a player scoring a goal given he was playiong at the time\n",
    "    - $\\psi_p$ - a Bernoulli distribution representing the probability of a player providing an assist given he was playing at the time\n",
    "- Define prior distributions over the parameters of the above distributions and update them using the match observations $o$ to obtain new posterior distributions.\n",
    "- Use simple closed-form equations e.g. Beta and Dirichlet conjugate priors to update the priors.\n",
    "- Sample from these conjugate distributions to generate $\\tau_p$.\n",
    "- Define hyperparemeters uniformly across all players i.e. $$\\omega_p \\sim Beta(1, 1), \\psi_p \\sim Beta(1, 1),  \\rho_p \\sim Dirichlet(\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4})$$\n",
    "- Potential to use performance data from previous seasons to define priors\n",
    "- Define 4 global multinomial distributions $S_{pos}$ - one for each position - to describe the distribution of minutes players who play the same position $pos$ are likely to play in a match, given they start the match.\n",
    "- Player absence via injury/suspension or any other reson is modelled by setting the probability of starting and substituting to zero i.e. $Pr(\\rho_p = start) \\text{and} Pr(\\rho_p = sub) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formulating the belief-state MDP\n",
    "- The belief state at gameweek $i$, $b_i$, is an instantiation of our belief model, updated with the match observations $O_{i - 1}$.\n",
    "- We observe the posterior player characteristics by updating the belief state in response to an observation $o \\in O_i$ via the Bayes rule: $$Pr(\\tau | b_{i + 1}) \\propto Pr(o | \\tau)Pr(\\tau | b_i)$$\n",
    "- The agent can perform optimally by maximizing the value of the Bellman equation: $$V(b_i) = \\max_{a \\in A_i} Q(b_i, a)$$\n",
    "- The Q-function is defined as: $$Q(b_i, a) = \\int_{\\tau} Pr(\\tau | b_i)  \\int_{o \\in O_i} Pr(o | \\tau) \\left[r_i + \\gamma V(b_{i + 1}) \\right] \\text{dod}\\tau$$\n",
    "- Where:\n",
    "    - $\\gamma \\in [0, 1)$ is the discount factor for future rewards\n",
    "    - $r_i = R(o, a_{prev}, a)$ is the reward function\n",
    "    - $V(b_{i + 1})$ is the value of the next belief state\n",
    "- Solutions to the Bellman equation is intractable due to the size of the outcome space $|O_i|$, the size of the action space $|A_i|$, and the need to consider up to 38 gameweeks in order to calculate Q-values exactly.\n",
    "- We can work around this sampling from $O_i$ and simulating match outcomes to approximate the Q-function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling Outcomes\n",
    "- We describe a model for sampling outcomes for gameweek $i$ from $Pr(O_i | \\tau)$. This is then combined with the belief model described beforehand to obtain a joint distribution of player abilities and match outcomes, thus treating uncertainty in player abilities in a Bayesian manner (observations) $$Pr(O_i | \\tau)Pr(\\tau | b_i)$$\n",
    "- Sampling procedure for a single match that also extends to any other match in the gameweek (it also takes the perspective of the home team, which naturally extends to the away team as well):\n",
    "    - Define $P_H$ and $P_A$ as the set of players available the home and away teams respectively.\n",
    "    - Sample $\\tau_p$ for each player $p \\in P_H$  from the belief model $Pr(\\tau_p | b_i)$\n",
    "    - Randomly select eleven players from $P_H$ in proportion to their probability of starting the match i.e. $Pr(\\rho_p = start)$\n",
    "        - These players constitute the starting lineup $L_H$\n",
    "    - The minute each player leaves the pitch is sampled from the $S_{pos}$ distribution for the player's position\n",
    "    - Each player in $P_H$ and not in $L_H$ is assigned to the set of substitutes $U_H$\n",
    "        - At the start of each minute of the match, we check if any player in $L_H$ is scheduled to be substituted\n",
    "        - If so, we randomly select a player from $U_H$ to replace the outgoing player in proportion to the probability of the player being substituted i.e. $Pr(\\rho_p = sub)$\n",
    "        - The replacement is added to $L_H$ (removed from $U_H$). We further assume that the player being substituted is not substituted again in the same match.\n",
    "        - If a goal is scored according to the underlying team-based model, then it is allocated to player $p$ with probability $Pr(\\omega_p = 1)$ while an assist is allocated to player $p$ with probability $Pr(\\psi_p = 1)$.\n",
    "    - These point estimates may then be used in combination with the MDP reward function $R$ to approximate the immediate reward from performing any action, as well as to guide the exploration of high quality regions of the action space.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
